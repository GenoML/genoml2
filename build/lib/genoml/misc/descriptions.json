{
  "check_dependencies": {
    "title": "Dependency Check",
    "description": "",
    "end": true,
    "error": ""
  },
  "check_dependencies_Plink": {
    "title": "Checking plink",
    "description": "",
    "error": ""
  },
  "data_summary": {
    "title": "Input Data Summary",
    "description": "Your data looks like this (showing summaries of the leftmost and rightmost columns)...\n\n{data}",
    "error": ""
  },
  "read_munge": {
    "title": "Reading Input File: {path}",
    "description": "",
    "error": ""
  },
  "export_model": {
    "title": "Exporting Model: {output_path}",
    "description": "this model has been saved as {output_path} for later use and can be found in your working directory.",
    "end": true,
    "error": ""
  },
  "info": {
    "title": "GenoML",
    "description": "{cmd}",
    "end": true,
    "error": ""
  },
  "munging/info": {
    "title": "Basic Info",
    "description": "\nHere is some basic info on the command you are about to run.\nPython version info...\n{python_version}\nCLI argument info...\nThe output prefix for this run is {prefix} and will be appended to later runs of GenoML.\nWorking with genotype data? {geno_path}\nWorking with additional predictors? {addit_path}\nWhere is your phenotype file? {pheno_path}\nWorking with genotype data for testing? {geno_test_path}\nWorking with additional predictors for testing? {addit_test_path}\nWhere is your phenotype file for testing? {pheno_test_path}\nDo you want GenoML skip pruning SNPs for you? {skip_prune}\nThe pruning threshold you've chosen is {r2}\nAny use for an external set of GWAS summary stats? {gwas_paths}\nIf you plan on using external GWAS summary stats for SNP filtering, we'll only keep SNPs at what P value? {p_gwas}\nHow strong is your VIF filter? {vif_thresh}\nHow many iterations of VIF filtering are you doing? {vif_iter}\nThe imputation method you picked is using the column {impute_type} to fill in any remaining NAs.\nWill you be adjusting additional features using UMAP dimensionality reduction? {umap_reduce}\nGive credit where credit is due, for this stage of analysis we use code from the great contributors to python packages: os, sys, argparse, numpy, pandas, joblib, math and time. We also use PLINK v1.9 from https://www.cog-genomics.org/plink/1.9/.\n",
    "error": ""
  },
  "harmonizing/info": {
    "title": "Basic Info",
    "description": "\nHere is some basic info on the command you are about to run.\nPython version info...\n{python_version}\nCLI argument info...\nThe output prefix for this run is {prefix} and will be appended to later runs of GenoML.\nWorking with genotype data for testing? {geno_path}\nWorking with additional predictors for testing? {addit_path}\nWhere is your phenotype file for testing? {pheno_path}\nApplying some of the same parameters that were used during training:\nHow strong is your VIF filter? {vif_thresh}\nHow many iterations of VIF filtering are you doing? {vif_iter}\nThe imputation method you picked is using the column {impute_type} to fill in any remaining NAs.\nYou have chosen {force_impute}to force-impute columns that were included during training but are not present in your harmonization dataset.\nWill you be adjusting additional features using UMAP dimensionality reduction? {umap_reduce}\nGive credit where credit is due, for this stage of analysis we use code from the great contributors to python packages: os, sys, argparse, numpy, pandas, joblib, math and time. We also use PLINK v1.9 from https://www.cog-genomics.org/plink/1.9/.\n",
    "error": ""
  },
  "training/info": {
    "title": "Basic Info",
    "description": "Here is some basic info on the command you are about to run.\nPython version info:\n{python_version}\n\nWorking with dataset from previous data munging efforts at:\n\t{prefix}\nYou have chosen to compete the algorithms based on {metric_max}.\nGive credit where credit is due, for this stage of analysis we use code from the great contributors to Python packages: argparse, xgboost, sklearn, pandas, numpy, time, matplotlib and seaborn.\nAs a note, in all exported probabilities and other graphics, case status is treated as a 0 or 1, with 1 representing a positive case.",
    "error": ""
  },
  "tuning/info": {
    "title": "Basic Info",
    "description": "Here is some basic info on the command you are about to run.\nPython version info:\n{python_version}\n\nArgument info...\nWorking with the dataset and best model corresponding to prefix {run_prefix} the timestamp from the merge is the prefix in most cases.\nYour maximum number of tuning iterations is {max_iter} and if you are concerned about runtime, make this number smaller.\nYou are running {cv_count} rounds of cross-validation, and again... if you are concerned about runtime, make this number smaller.\nGive credit where credit is due, for this stage of analysis we use code from the great contributors to python packages: argparse, xgboost, sklearn, pandas, numpy, time, matplotlib and seaborn.\nAs a note, in all exported probabilities and other graphics, case status is treated as a 0 or 1, with 1 representing a positive case.\n",
    "error": ""
  },
  "testing/info": {
    "title": "Basic Info",
    "description": "Here is some basic info on the command you are about to run.\nPython version info:\n{python_version}\n\nArgument info...\nYou are importing this test dataset: {prefix}.\n",
    "error": ""
  },
  "utils/matching_columns_path": {
    "title": "",
    "description": "Looks like you are retraining your reference file. We are using the harmonized columns you provided here: {matching_columns_path}\nNote that you might have different/less features than before, given this was harmonized between training and test dataset, and might mean your model now performs worse...",
    "error": ""
  },
  "utils/training/compete": {
    "title": "Compete the algorithms",
    "description": "Now let's compete these algorithms!\nWe'll update you as each algorithm runs, then summarize at the end.\nHere we test each algorithm under default settings using the same training and test datasets derived from a 70% training and 30% testing split of your data.\nFor each algorithm, we will output the following metrics...\nAlgorithm name, hoping that's pretty self-explanatory. Plenty of resources on these common ML algorithms at https://scikit-learn.org and https://xgboost.readthedocs.io/.\nexplained_variance_score, this is the variance explained by the model per algorithm (scale from 0 to 1 with 1 being completely explained).\nmean_squared_error, this is the mean squared error from regression loss.\nmedian_absolute_error, median absolute error from regression loss.\nr2_score, standard r2 metric from linear regression (coefficient of determination), remember, this can be negative if your model is really bad.\nWe also log the runtimes per algorithm.\n\nAlgorithm summaries incoming...",
    "end": true,
    "error": ""
  },
  "utils/training/compete/algorithm/best": {
    "title": "Best Algorithm: {algorithm}",
    "description": "There are occasionally slight fluctuations in model performance on the same withheld samples.\n{metrics}",
    "error": ""
  },
  "utils/training/fit_algorithms/compete/algorithm": {
    "title": "{name}",
    "description": "",
    "error": ""
  },
  "utils/training/fit_algorithms/compete/algorithm/results": {
    "title": "{name} Results",
    "description": "{results}",
    "error": ""
  },
  "utils/training/fit_algorithms/compete/save_algorithm_results": {
    "title": "Saving Algorithm Results: {output_path}",
    "description": "This table below is also logged as {output_path} and is in your current working directory...\n\n{data}",
    "end": true,
    "error": ""
  },
  "utils/export_predictions": {
    "title": "Saving Predictions: {output_path}",
    "description": "Preview of the exported predictions for the training samples which is naturally overfit and exported as {output_path} in the similar format as in the withheld test dataset that was just exported.\n\n{data}",
    "end": true,
    "error": ""
  },
  "utils/export_predictions/withheld_data": {
    "title": "Saving Predictions on withheld data: {output_path}",
    "description": "Preview of the exported predictions for the withheld validation data that has been exported as {output_path} these are pretty straight forward.\nThey generally include the sample ID, the previously reported phenotype and the predicted phenotype from that algorithm,\n\n{data}",
    "end": true,
    "error": ""
  },
  "utils/export_predictions/plot": {
    "title": "Saving Regression Plot: {output_path}",
    "description": "Here is a quick summary of the regression comparing PHENO_REPORTED ~ PHENO_PREDICTED in the withheld test data...\n{data}\n...always good to see the P for the predictor.\n\nWe are also exporting a regression plot for you here {output_path} this is a graphical representation of the difference between the reported and predicted phenotypes in the withheld test data for the best performing algorithm.",
    "end": true,
    "error": ""
  }
}
